mode: train
gpu_ids: [0]
scale: 4 #todo
run_range: 1
mask_training: mask_collate_fn_arbrpn


datasets:
    train_QB: #todo train datasets 11bit
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../dataset/NBU_QB_500samples/train
        scaledict:
            LR: 1
            REF: 4
            GT: 4
        img_range: 2047
        LRdim: 4
        REFdim: 1
        n_workers: 4
        repeat: 4
        batch_size: 8   #todo:
        patch_size: 16
        use_flip: true
        use_rot: true
        noise: .

      
    train_GF1: #todo train datasets 10bit
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../dataset/NBU_GF1_410samples/train
        scaledict:
            LR: 1
            REF: 4
            GT: 4
        img_range: 1023
        LRdim: 4
        REFdim: 1
        n_workers: 4
        repeat: 4
        batch_size: 8   #todo:
        patch_size: 16
        use_flip: true
        use_rot: true
        noise: .

    valid_QB: #todo validation datasets
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../dataset/NBU_QB_500samples/valid
        img_range: 2047
        get_MTF: True
        scaledict:
            LR: 1
            REF: 4
            GT: 4
        LRdim: 4
        REFdim: 1
        batch_size: 1
        n_workers: 4 
        noise: .


    valid_GF1: #todo validation datasets
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../dataset/NBU_GF1_410samples/valid
        img_range: 1023
        get_MTF: True
        scaledict:
            LR: 1
            REF: 4
            GT: 4
        LRdim: 4
        REFdim: 1
        batch_size: 1
        n_workers: 4 
        noise: .
        
## hyper-parameters for network architecture
networks:
    net_arch: linear_transformer # this v alue must be same with the filename of 'your_network_name'.py
    numLayers: 4
    convDim: 2
    numHeads: 4
    patchSize: 1
    poolSize: 4
    # learning_rate: 0.0002

# the setting for optimizer, loss function, learning_strategy, etc.
solver:
    loss_name: l1
    optimType: ADAM
    learning_rate: 0.0002
    lr_scheme: warm_up # warm_up or multisteplr
    warmUpEpoch: 0 # for warm_up
    lrStepSize: 200 # for multisteplr
    weight_decay: null #todo: 0.0001
    acuSteps: 1 #todo:
    manual_seed: 0
    num_epochs: 3
    save_ckp_step: 1000
    pretrain: null
    pretrained_path: experiments/FinalAblation/3_base_conv_patchv2_transformer_B8P18_lr0002_warm_up/epochs/last_ckp.pth

logger:
    name: 
    tags: [MixGQW, Norm2047] #ablation, Hyper
    log_dir: Ablation_study_QBFIX/
