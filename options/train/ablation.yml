mode: train
gpu_ids: [0]
scale: 4 #todo
run_range: 1
mask_training: mask_collate_fn


datasets:
    train_WV2-MIX8: #todo train datasets
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../PanSharp_dataset/WV2/train/MTF/MIX_8_rand
        img_range: 2047
        LRdim: 8
        REFdim: 1
        n_workers: 4
        repeat: 3
        batch_size: 1   #todo:
        patch_size: 16
        use_flip: true
        use_rot: true
        noise: .

    valid_WV2-MIX8: #todo validation datasets
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../PanSharp_dataset/WV2/valid/MTF/MIX_8_rand
        img_range: 2047
        LRdim: 8
        REFdim: 1
        batch_size: 1
        n_workers: 4 
        noise: .
        
## hyper-parameters for network architecture
networks:
    net_arch: linear_transformer # this v alue must be same with the filename of 'your_network_name'.py
    numLayers: 4
    convDim: 2
    numHeads: 4
    patchSize: 1
    poolSize: 4
    # learning_rate: 0.0002

# the setting for optimizer, loss function, learning_strategy, etc.
solver:
    optimType: ADAM
    learning_rate: 0.0002
    lr_scheme: warm_up # warm_up or multisteplr
    warmUpEpoch: 0 # for warm_up
    lrStepSize: 200 # for multisteplr
    weight_decay: null #todo: 0.0001
    acuSteps: 8 #todo:
    manual_seed: 0
    num_epochs: 200
    save_ckp_step: 1000
    pretrain: null
    pretrained_path: experiments/FinalAblation/3_base_conv_patchv2_transformer_B8P18_lr0002_warm_up/epochs/last_ckp.pth

logger:
    name: 
    tags: [Ablation, arbtrans_base] #ablation, Hyper
    log_dir: Ablation_study/
